{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import lxml\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalize_each_word(str):\n",
    "    list_of_words = str.split()\n",
    "    final_list = []\n",
    "    for word in list_of_words:\n",
    "        proccessed_word = word.capitalize()\n",
    "        final_list.append(proccessed_word)\n",
    "    capitalized_state = (\" \".join(final_list))\n",
    "    return capitalized_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {'Maharashtra':'MH', 'Karnataka': 'KA', 'Kerala':'KL','Uttar Pradesh':'UP','Tamil Nadu':'TN','Andhra Pradesh':'AP',\n",
    "             'Delhi':'DL','West Bengal':'WB','Chhattisgarh':'CT', 'Rajasthan':'RJ', 'Gujarat':'GJ', 'Madhya Pradesh':'MP', \n",
    "             'Haryana':'HR','Bihar':'BR','Odisha':'OR','Telangana':'TG','Punjab':'PB','Assam':'AS','Jharkhand':'JH','Uttarakhand'\n",
    "             :'UT','Jammu And Kashmir':'JK','Himachal Pradesh':'HP','Goa':'GA,','Puducherry':'PY','Chandigarh':'CH','Tripura':'TR',\n",
    "             'Manipur':'MN','Meghalaya':'ML','Arunachal Pradesh':'AR','Nagaland':'NL','Ladakh':'LA','Sikkim':'SK','Daman And Diu':'DN',\n",
    "             'Mizoram':'MZ','Andaman And Nicobar Islands':'AN','Lakshadweep':'LD'}\n",
    "# State dictionary to find the URL of that particular website "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException        \n",
    "def check_exists_by_xpath(xpath):\n",
    "    try:\n",
    "        driver.find_element_by_xpath(xpath)\n",
    "    except NoSuchElementException:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter State Name\n",
      "andhra pradesh\n",
      "https://www.covid19india.org/state/AP\n"
     ]
    }
   ],
   "source": [
    "state = input(\"Enter State Name\\n\")\n",
    "#Taking state name as input and using state_dict to find the code for that state\n",
    "\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--disable-gpu')\n",
    "#using selenium to set to a headless browser\n",
    "\n",
    "chromedriver_path= \"C:\\\\Users\\Manav Doshi\\Documents\\WnCC Convenership\\chromedriver.exe\"\n",
    "#initialising path of chromedriver\n",
    "\n",
    "driver = webdriver.Chrome(chromedriver_path, options=options)\n",
    "#using path to open the browser \n",
    "\n",
    "url = \"https://www.covid19india.org/state/\"+state_dict[capitalize_each_word(state)]\n",
    "print(url)\n",
    "#finding the URL of that specific webpage\n",
    "\n",
    "driver.get(url)\n",
    "#using the URL and browser to gather the dynamic content\n",
    "\n",
    "time.sleep(3)\n",
    "#if you want to wait 3 seconds for the page to load\n",
    "bool = (check_exists_by_xpath('//button[@class=\"button fadeInUp\"]'))\n",
    "\n",
    "#check if the view more button actually exists\n",
    "\n",
    "if(bool):\n",
    "    submit_button = driver.find_element_by_xpath('//button[@class=\"button fadeInUp\"]')\n",
    "    submit_button.click()\n",
    "#since all the district data only shows up when \"View More\" is clicked simulating the click here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_source = driver.page_source\n",
    "#Getting the source\n",
    "\n",
    "soup = BeautifulSoup(page_source, 'lxml')\n",
    "#parsing the HTML content of the webpage \n",
    "# print(soup.find('div', class_ = 'districts').prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "#Wait for 5 seconds for the page to completely load \n",
    "fields = ['District','Cases']\n",
    "district_stats = [[]]\n",
    "for first_num in soup.find_all('div',class_ = 'district'):\n",
    "    num=first_num.h2.text\n",
    "    name = first_num.h5.text\n",
    "    district_stats.append([name,num])\n",
    "#start looping through every h2 in the div with class of district which contains the number of cases\n",
    "#h5 contains the name of that district "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('covid_data.csv','w', newline = '') as file:\n",
    "    writer=csv.writer(file)\n",
    "    writer.writerow(fields)\n",
    "    for row in district_stats:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "9938ab0846f9bb2ebc1402c53752e0c9e4a305dfc6cf35718a7a69bfe52f1365"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
